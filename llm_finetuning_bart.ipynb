{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gheto\\miniconda3\\envs\\aromanianLLM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\gheto\\miniconda3\\envs\\aromanianLLM\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "\n",
    "model_name = \"facebook/bart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your translations.csv\n",
    "df = pd.read_csv(\"../dataset/nllb_corpus_train.csv\")\n",
    "eval_df = pd.read_csv(\"../dataset/nllb_corpus_test.csv\")\n",
    "# Rename columns if needed\n",
    "df.columns = ['ro', 'rup']\n",
    "# Convert to Hugging Face Dataset format\n",
    "dataset = Dataset.from_pandas(df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/27033 [00:00<?, ? examples/s]c:\\Users\\gheto\\miniconda3\\envs\\aromanianLLM\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 27033/27033 [00:02<00:00, 9474.01 examples/s] \n",
      "Map: 100%|██████████| 3004/3004 [00:00<00:00, 8516.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples, max_length=128):\n",
    "    inputs = examples['ro']\n",
    "    targets = examples['rup']\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Tokenize the target (Aromanian) with the target tokenizer\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing to the entire dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gheto\\miniconda3\\envs\\aromanianLLM\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/10140 [00:00<?, ?it/s]c:\\Users\\gheto\\miniconda3\\envs\\aromanianLLM\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:496: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "  5%|▍         | 500/10140 [02:34<48:50,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1083, 'grad_norm': 1.3328168392181396, 'learning_rate': 4.7534516765285995e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1000/10140 [05:03<45:34,  3.34it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1957, 'grad_norm': 0.7041230797767639, 'learning_rate': 4.5069033530571994e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1500/10140 [07:38<43:13,  3.33it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1755, 'grad_norm': 0.5219510197639465, 'learning_rate': 4.260355029585799e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 2000/10140 [10:04<39:53,  3.40it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1733, 'grad_norm': 0.5312537550926208, 'learning_rate': 4.0138067061143986e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2500/10140 [12:36<36:57,  3.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1759, 'grad_norm': 0.4619382917881012, 'learning_rate': 3.767258382642998e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 3000/10140 [15:07<37:02,  3.21it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.163, 'grad_norm': 0.4619596600532532, 'learning_rate': 3.520710059171598e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 33%|███▎      | 3380/10140 [17:39<29:33,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16245995461940765, 'eval_runtime': 27.9783, 'eval_samples_per_second': 107.369, 'eval_steps_per_second': 13.439, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 3500/10140 [18:16<34:06,  3.24it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1574, 'grad_norm': 0.6137378215789795, 'learning_rate': 3.2741617357001976e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 4000/10140 [20:52<32:41,  3.13it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1455, 'grad_norm': 0.8199504017829895, 'learning_rate': 3.027613412228797e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4500/10140 [23:36<30:09,  3.12it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1447, 'grad_norm': 1.044874906539917, 'learning_rate': 2.7810650887573965e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 5000/10140 [26:14<26:59,  3.17it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1421, 'grad_norm': 0.706843376159668, 'learning_rate': 2.5345167652859964e-05, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5500/10140 [29:01<25:00,  3.09it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1372, 'grad_norm': 0.5742181539535522, 'learning_rate': 2.287968441814596e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 6000/10140 [31:37<20:23,  3.38it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.131, 'grad_norm': 0.694061279296875, 'learning_rate': 2.0414201183431952e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 6500/10140 [34:12<17:32,  3.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1348, 'grad_norm': 0.7428207993507385, 'learning_rate': 1.794871794871795e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 67%|██████▋   | 6760/10140 [35:54<14:22,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1432987004518509, 'eval_runtime': 27.3978, 'eval_samples_per_second': 109.644, 'eval_steps_per_second': 13.724, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 7000/10140 [37:05<15:03,  3.47it/s]  Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1245, 'grad_norm': 0.5974081158638, 'learning_rate': 1.5483234714003947e-05, 'epoch': 2.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 7500/10140 [39:46<13:34,  3.24it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1156, 'grad_norm': 0.6151213049888611, 'learning_rate': 1.3017751479289941e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 8000/10140 [42:21<10:56,  3.26it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1187, 'grad_norm': 0.5143307447433472, 'learning_rate': 1.0552268244575937e-05, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 8500/10140 [45:00<08:25,  3.24it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1173, 'grad_norm': 0.576835572719574, 'learning_rate': 8.086785009861933e-06, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 9000/10140 [47:30<05:29,  3.46it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1185, 'grad_norm': 0.5130419135093689, 'learning_rate': 5.621301775147929e-06, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 9500/10140 [50:06<03:08,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.115, 'grad_norm': 0.7469008564949036, 'learning_rate': 3.1558185404339255e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 10000/10140 [52:34<00:43,  3.23it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1106, 'grad_norm': 0.8834843635559082, 'learning_rate': 6.903353057199211e-07, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10140/10140 [53:25<00:00,  3.86it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "                                                     \n",
      "100%|██████████| 10140/10140 [53:59<00:00,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13884691894054413, 'eval_runtime': 28.8257, 'eval_samples_per_second': 104.213, 'eval_steps_per_second': 13.044, 'epoch': 3.0}\n",
      "{'train_runtime': 3239.2661, 'train_samples_per_second': 25.036, 'train_steps_per_second': 3.13, 'train_loss': 0.18929129023053473, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10140, training_loss=0.18929129023053473, metrics={'train_runtime': 3239.2661, 'train_samples_per_second': 25.036, 'train_steps_per_second': 3.13, 'total_flos': 2.1968752015835136e+16, 'train_loss': 0.18929129023053473, 'epoch': 3.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,  \n",
    "    eval_dataset=tokenized_eval_dataset,  \n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mahnit adancu, cari s-dutea,\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence):\n",
    "    inputs = tokenizer([sentence], return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "    inputs = inputs.to(model.device)\n",
    "    output = model.generate(**inputs)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "ro_sentence = \"Mahnit adanc, porni spre casa, rusinat de neizbanda lui, de parca se ducea la taierea capului.\"\n",
    "print(translate(ro_sentence))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ro_rup_test = pd.read_csv(\"../dataset/nllb_corpus_test.csv\")\n",
    "df_ro_rup_test['ro_pred'] = ''\n",
    "df_ro_rup_test['rup_pred'] = ''\n",
    "\n",
    "test_len = len(df_ro_rup_test)\n",
    "test_len = 200\n",
    "for i in tqdm(range(0, test_len)):\n",
    "    ro_texts = df_ro_rup_test.loc[i, 'ro']\n",
    "    \n",
    "    if ro_texts:\n",
    "        df_ro_rup_test.loc[i, 'rup_pred'] = translate(ro_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 1.28 9.5/4.7/0.3/0.2 (BP = 1.000 ratio = 1.381 hyp_len = 370 ref_len = 268)\n",
      "chrF2 = 19.90\n"
     ]
    }
   ],
   "source": [
    "bleu_calc = sacrebleu.BLEU()\n",
    "chrf_calc = sacrebleu.CHRF()\n",
    "print(bleu_calc.corpus_score(df_ro_rup_test['rup'][:200].to_list(), [df_ro_rup_test['rup_pred'][:200].to_list()]))\n",
    "print(chrf_calc.corpus_score(df_ro_rup_test['rup'][:200].to_list(), [df_ro_rup_test['rup_pred'][:200].to_list()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aromanianLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
