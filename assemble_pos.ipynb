{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('ro_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_dict = dict()\n",
    "#Alinieri cu eflomal\n",
    "def read_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return [line.strip() for line in file]\n",
    "\n",
    "def parse_alignments(alignment_file, source_sentences, target_sentences):\n",
    "    with open(alignment_file, 'r', encoding='utf-8') as file:\n",
    "        for line_num, line in enumerate(file):\n",
    "            alignment_pairs = line.strip().split()\n",
    "            for pair in alignment_pairs:\n",
    "                src_index, tgt_index = map(int, pair.split('-'))\n",
    "                src_word = source_sentences[line_num].split()[src_index]\n",
    "                tgt_word = target_sentences[line_num].split()[tgt_index]\n",
    "                yield src_word, tgt_word\n",
    "\n",
    "# Read sentences from source and target files\n",
    "source_sentences = read_sentences('corpus.rup')\n",
    "target_sentences = read_sentences('corpus.ro')\n",
    "\n",
    "# Extract and print aligned word pairs\n",
    "for src_word, tgt_word in parse_alignments('forward.align', source_sentences, target_sentences):\n",
    "    print(f\"{src_word} - {tgt_word}\")\n",
    "    alignment_dict[src_word] = tgt_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "excel_path = \"Papahagi.xls\"\n",
    "df = pd.read_excel(excel_path, header = None)\n",
    "\n",
    "df.columns = [\"POS\", \"aro\", \"ro\", \"origine\", \"IDK\", \"autor\"]\n",
    "df.head()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "with open('corpus.rup', 'r', encoding='utf-8') as file:\n",
    "    aro_text = file.read()\n",
    "\n",
    "aro_tokens = word_tokenize(aro_text.lower())\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "direct_translation = dict()\n",
    "for word in tqdm.tqdm(aro_tokens):\n",
    "    if word in df[\"aro\"].values.tolist():\n",
    "        # print(word)\n",
    "        # print(df[df[\"aro\"] == word][\"ro\"].values.tolist())\n",
    "        direct_translation[word] = df[df[\"aro\"] == word][\"ro\"].values.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus.ro\", \"r\", encoding=\"utf-8\") as romanian_file:\n",
    "    romanian_text = romanian_file.read()\n",
    "\n",
    "with open(\"corpus.rup\", \"r\", encoding=\"utf-8\") as aromanian_file:\n",
    "    aromanian_text = aromanian_file.read()\n",
    "\n",
    "doc_ro = nlp(romanian_text)\n",
    "\n",
    "romanian_tokens = [token.text for token in doc_ro]\n",
    "romanian_pos_tags = [token.pos_ for token in doc_ro]\n",
    "\n",
    "aromanian_tokens = aromanian_text.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aromanian_pos_tags = []\n",
    "total_words = 0\n",
    "unknown_words = 0\n",
    "import tqdm\n",
    "\n",
    "\n",
    "total_words = 0\n",
    "unknown_words = 0\n",
    "\n",
    "\n",
    "for token in tqdm.tqdm(aromanian_tokens):\n",
    "    total_words += 1\n",
    "    pos_tag_assigned = False\n",
    "\n",
    "    if token in alignment_dict:\n",
    "        romanian_equivalent = alignment_dict[token]\n",
    "        try:\n",
    "            index = romanian_tokens.index(romanian_equivalent)\n",
    "            aromanian_pos_tags.append(romanian_pos_tags[index])\n",
    "            pos_tag_assigned = True\n",
    "        except ValueError:\n",
    "            pass \n",
    "\n",
    "    if not pos_tag_assigned and token in direct_translation:\n",
    "        romanian_direct_equivalent = direct_translation[token]\n",
    "        try:\n",
    "            index = romanian_tokens.index(romanian_direct_equivalent)\n",
    "            aromanian_pos_tags.append(romanian_pos_tags[index])\n",
    "            pos_tag_assigned = True\n",
    "        except ValueError:\n",
    "            pass  \n",
    "\n",
    "    if not pos_tag_assigned:\n",
    "        doc_ro_token = nlp(token)\n",
    "        if len(doc_ro_token) > 0:\n",
    "            aromanian_pos_tags.append(doc_ro_token[0].pos_)\n",
    "        else:\n",
    "            unknown_words += 1\n",
    "            aromanian_pos_tags.append(\"UNKNOWN\")\n",
    "\n",
    "\n",
    "print(f\"Total words processed: {total_words}\")\n",
    "print(f\"Unknown words: {unknown_words}\")\n",
    "\n",
    "\n",
    "\n",
    "for token, pos_tag in zip(aromanian_tokens, aromanian_pos_tags):\n",
    "    print(f\"{token} ({pos_tag})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(aromanian_tokens), len(aromanian_pos_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model cu algoritm Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aromanian_tokens = aromanian_tokens\n",
    "aromanian_pos_tags = [x if x != '' else 'X' for x in aromanian_pos_tags]\n",
    "aromanian_pos_tags = [x if x != \"PROPN\" else \"NOUN\" for x in aromanian_pos_tags]\n",
    "\n",
    "train_tagged_words = [(aromanian_tokens[i], aromanian_pos_tags[i]) for i in range(len(aromanian_tokens))]\n",
    "\n",
    "print(train_tagged_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set(aromanian_pos_tags)\n",
    "print(len(tags))\n",
    "print(tags)\n",
    "vocab = set(aromanian_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_tagged_words))\n",
    "print(len(set(aromanian_tokens)))\n",
    "print(len(aromanian_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_given_tag = len(tag_list)#total number of times the passed tag occurred in train_bag\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "#now calculate the total number of times the passed word occurred as the passed tag.\n",
    "    count_given_w_with_given_tag = len(w_given_tag_list)\n",
    " \n",
    "     \n",
    "    return (count_given_w_with_given_tag, count_given_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Transition Probability\n",
    "def t2_given_t1(tag2, tag1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==tag1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==tag1 and tags[index+1] == tag2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags, t= no of tags\n",
    "# Matrix(i, j) represents P(jth tag after the ith tag)\n",
    " \n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "    for j, t2 in enumerate(list(tags)): \n",
    "        aux = t2_given_t1(t2, t1)\n",
    "        tags_matrix[i, j] = aux[0]/aux[1]\n",
    " \n",
    "#print(tags_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\n",
    "display(tags_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "     \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['PUNCT', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                 \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "             \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_la_misto = \"Sănt îndoauă învețuri, cari si amintă mași cu ghivăsirea, i cu avdzărea.\"\n",
    "test_la_misto = test_la_misto.split()\n",
    "\n",
    "testt = Viterbi(test_la_misto)\n",
    "\n",
    "print(testt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testare pentru Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(text):\n",
    "    \n",
    "    lines = text.strip().split('\\n')\n",
    "\n",
    "    annotations = []\n",
    "\n",
    " \n",
    "    for line in lines:\n",
    "        if line.strip(): \n",
    "            word, tag = line.split()  \n",
    "            if tag == \"PUNCT\":\n",
    "                continue\n",
    "            annotations.append((word, tag))  \n",
    "\n",
    "    return annotations\n",
    "\n",
    "def parse_annotations_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        return parse_annotations(text)\n",
    "    except FileNotFoundError:\n",
    "        return \"File not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "def calculate_accuracy(actual_annotations, predicted_annotations):\n",
    "    # Check if both lists are of the same length\n",
    "    if len(actual_annotations) != len(predicted_annotations):\n",
    "        print(len(actual_annotations), len(predicted_annotations))\n",
    "        return \"The number of actual and predicted annotations must be the same.\"\n",
    "\n",
    "    correct_predictions = sum(1 for actual, predicted in zip(actual_annotations, predicted_annotations) if actual == predicted)\n",
    "\n",
    "    accuracy = correct_predictions / len(actual_annotations)\n",
    "    return accuracy\n",
    "def calculate_f1_score(actual_annotations, predicted_annotations):\n",
    "    # Check if both lists are of the same length\n",
    "    if len(actual_annotations) != len(predicted_annotations):\n",
    "        return \"The number of actual and predicted annotations must be the same.\"\n",
    "\n",
    "    true_positives = sum(1 for actual, predicted in zip(actual_annotations, predicted_annotations) if actual == predicted)\n",
    "    false_positives = sum(1 for actual, predicted in zip(actual_annotations, predicted_annotations) if actual != predicted and predicted != \"PUNCT\")\n",
    "    false_negatives = sum(1 for actual, predicted in zip(actual_annotations, predicted_annotations) if actual != predicted and actual != \"PUNCT\")\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "file_path = 'test_ann.txt'\n",
    "\n",
    "actual_annotations = parse_annotations_from_file(file_path)\n",
    "\n",
    "\n",
    "words_to_predict = [word for word, tag in actual_annotations if tag != \"PUNCT\"]\n",
    "predicted_tags = Viterbi(words_to_predict)\n",
    "print(predicted_tags)\n",
    "\n",
    "# Calculate and print the accuracy, if actual annotations were successfully parsed\n",
    "if isinstance(actual_annotations, list):\n",
    "    accuracy = calculate_accuracy(actual_annotations, predicted_tags)\n",
    "    print(accuracy)\n",
    "    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "else:\n",
    "    print(actual_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(min(len(actual_annotations), len(predicted_tags))):\n",
    "    if actual_annotations[i] != predicted_tags[i]:\n",
    "        print(actual_annotations[i], predicted_tags[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testare pentru alinieri cu elfomal, giza++ si asocieri directe in dictionar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = 'test_seen_ann.txt'\n",
    "actual_annotations = parse_annotations_from_file(file_path)\n",
    "words_to_predict = [word for word, tag in actual_annotations if tag != \"PUNCT\"]\n",
    "\n",
    "with open(\"corpus.ro\", \"r\", encoding=\"utf-8\") as romanian_file:\n",
    "    romanian_text = romanian_file.read()\n",
    "\n",
    "with open(\"corpus.rup\", \"r\", encoding=\"utf-8\") as aromanian_file:\n",
    "    aromanian_text = aromanian_file.read()\n",
    "\n",
    "doc_ro = nlp(romanian_text)\n",
    "\n",
    "romanian_tokens = [token.text for token in doc_ro]\n",
    "romanian_pos_tags = [token.pos_ for token in doc_ro]\n",
    "\n",
    "aromanian_tokens = words_to_predict\n",
    "\n",
    "\n",
    "aromanian_pos_tags = []\n",
    "total_words = 0\n",
    "unknown_words = 0\n",
    "\n",
    "\n",
    "import tqdm\n",
    "for token in tqdm.tqdm(aromanian_tokens):\n",
    "    total_words += 1\n",
    "    pos_tag_assigned = False\n",
    "\n",
    "    if token in alignment_dict:\n",
    "        romanian_equivalent = alignment_dict[token]\n",
    "        try:\n",
    "            index = romanian_tokens.index(romanian_equivalent)\n",
    "            aromanian_pos_tags.append(romanian_pos_tags[index])\n",
    "            pos_tag_assigned = True\n",
    "        except ValueError:\n",
    "            pass \n",
    "\n",
    "    if not pos_tag_assigned and token in direct_translation:\n",
    "        romanian_direct_equivalent = direct_translation[token]\n",
    "        try:\n",
    "            index = romanian_tokens.index(romanian_direct_equivalent)\n",
    "            aromanian_pos_tags.append(romanian_pos_tags[index])\n",
    "            pos_tag_assigned = True\n",
    "        except ValueError:\n",
    "            pass  \n",
    "\n",
    "    if not pos_tag_assigned:\n",
    "        doc_ro_token = nlp(token)\n",
    "        if len(doc_ro_token) > 0:\n",
    "            aromanian_pos_tags.append(doc_ro_token[0].pos_)\n",
    "        else:\n",
    "            unknown_words += 1\n",
    "            aromanian_pos_tags.append(\"UNKNOWN\")\n",
    "\n",
    "\n",
    "print(f\"Total words processed: {total_words}\")\n",
    "print(f\"Unknown words: {unknown_words}\")\n",
    "\n",
    "\n",
    "predicted_tags = []\n",
    "for token, pos_tag in zip(aromanian_tokens, aromanian_pos_tags):\n",
    "    predicted_tags.append((token, pos_tag))\n",
    "print(predicted_tags)\n",
    "for pred_tag in predicted_tags:\n",
    "    print(pred_tag)\n",
    "\n",
    "if isinstance(actual_annotations, list):\n",
    "    accuracy = calculate_accuracy(actual_annotations, predicted_tags)\n",
    "    f1 = calculate_f1_score(actual_annotations, predicted_tags)\n",
    "    print(accuracy)\n",
    "    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
    "else:\n",
    "    print(actual_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the rup directly in spacy using the romanian POS tagger\n",
    "# doc_rup = nlp(aromanian_text)\n",
    "print(words_to_predict)\n",
    "words_to_predict_nlp = nlp(\" \".join(words_to_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words_to_predict_nlp)\n",
    "predicted_tags = []\n",
    "for token in words_to_predict_nlp:\n",
    "    predicted_tags.append((str(token), token.pos_))\n",
    "print(predicted_tags)\n",
    "ac = calculate_accuracy(actual_annotations, predicted_tags[:-1])\n",
    "# print(actual_annotations)\n",
    "print(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = []\n",
    "possible_tags = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PRON\", \"ADP\", \"NUM\", \"DET\", \"CONJ\", \"PRT\", \"X\", \"INTJ\"]\n",
    "import random\n",
    "for token in words_to_predict_nlp:\n",
    "    predicted_tags.append((str(token), random.choice(possible_tags)))\n",
    "ac = calculate_accuracy(actual_annotations, predicted_tags[:-1])\n",
    "print(actual_annotations)\n",
    "print(ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = \"\"\"A fost - ce n-a mai fost.\n",
    "A fost odată un lup, - Cumătrul-Nicola, și-o vulpe, - Cumătra-Mara.\n",
    "Ăștia doi s-au întovărășit, să vâneze împreună, iar vânatul, și-au zis, l-or împărți frățește.\n",
    "Ieșind la vânat, dar, dau peste o oală cu miere.\n",
    "- Aha!, se repezi Cumătru-Nicola, - o pap!\n",
    "- Mai întâi și mai întâi nu trebuie să spui: „o pap!“, ci „o păpăm“, i-a zis Cumătra-Mara. Apoi, nu te gândești, nici n-am dat bine de oală, și gata: pe ea! Dacă vrei să facem casă laolaltă, află că n-o facem înghițind orice agonisim...\n",
    "- Fie, Mara, cum știi tu.\n",
    "- Da, cum știu eu! Să facem și noi ca toată lumea. Acum, bunăoară, ascundem mierea și din trei în trei zile mergem să mâncăm din ea.\"\"\"\n",
    "prop_nlp = nlp(prop)\n",
    "pos_tags = []\n",
    "for token in prop_nlp:\n",
    "    pos_tags.append((str(token), token.pos_))\n",
    "    print((str(token), token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
