{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Source](https://towardsdatascience.com/implementing-word2vec-in-pytorch-from-the-ground-up-c7fe5bf99889)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ZJS1M4c-oxn4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "from collections import Counter, OrderedDict\n",
        "from dataclasses import dataclass\n",
        "from time import monotonic\n",
        "from typing import Dict, List, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from scipy.spatial.distance import cosine\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import WikiText103\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aromanian_iterator(file_path, chunk_size=1024):\n",
        "    \"\"\"\n",
        "    Creates an iterator over the WikiText-103 file.\n",
        "\n",
        "    :param file_path: Path to the WikiText-103 file.\n",
        "    :param chunk_size: Number of characters to read in each iteration.\n",
        "    :return: Yields chunks of text from the file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        while True:\n",
        "            chunk = file.read(chunk_size)\n",
        "            if not chunk:\n",
        "                break\n",
        "            yield chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = 'C:/Users/gheto/Desktop/PoS/AromanianPoS/dataset/Tales.test.ro'\n",
        "for text_chunk in aromanian_iterator(file_path):\n",
        "    # Process each text chunk\n",
        "    print(text_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "shxXAotV4v5m"
      },
      "outputs": [],
      "source": [
        "def get_data(train_dir='C:/Users/gheto/Desktop/PoS/AromanianPoS/dataset/Tales.train.ro', valid_dir='C:/Users/gheto/Desktop/PoS/AromanianPoS/dataset/Tales.test.ro'):\n",
        "    # gets the data\n",
        "    train_iter = aromanian_iterator(train_dir)\n",
        "    train_iter = to_map_style_dataset(train_iter)\n",
        "    valid_iter = aromanian_iterator(valid_dir)\n",
        "    valid_iter = to_map_style_dataset(valid_iter)\n",
        "\n",
        "    return train_iter, valid_iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "id": "wKZNNoSto6GK"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Word2VecParams:\n",
        "\n",
        "    # skipgram parameters\n",
        "    MIN_FREQ = 1  # aici trebuie sa fie 1, altfel o sa fie token-uri in context care nu sunt si in center\n",
        "    SKIPGRAM_N_WORDS = 20\n",
        "    T = 85\n",
        "    NEG_SAMPLES = 1\n",
        "    NS_ARRAY_LEN = 5_000_000\n",
        "    SPECIALS = \"<unk>\"\n",
        "    TOKENIZER = 'basic_english'\n",
        "\n",
        "    # network parameters\n",
        "    BATCH_SIZE = 100\n",
        "    EMBED_DIM = 300\n",
        "    EMBED_MAX_NORM = None\n",
        "    N_EPOCHS = 100\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    CRITERION = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "FB5Kh3LCqBDE"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, list, specials):\n",
        "        self.stoi = {v[0]:(k, v[1]) for k, v in enumerate(list)}\n",
        "        self.itos = {k:(v[0], v[1]) for k, v in enumerate(list)}\n",
        "        self._specials = specials[0]\n",
        "        self.total_tokens = np.nansum(\n",
        "            [f for _, (_, f) in self.stoi.items()]\n",
        "            , dtype=int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.stoi) - 1\n",
        "\n",
        "    def get_index(self, word: Union[str, List]):\n",
        "        if isinstance(word, str):\n",
        "            if word in self.stoi:\n",
        "                return self.stoi.get(word)[0]\n",
        "            else:\n",
        "                return self.stoi.get(self._specials)[0]\n",
        "        elif isinstance(word, list):\n",
        "            res = []\n",
        "            for w in word:\n",
        "                if w in self.stoi:\n",
        "                    res.append(self.stoi.get(w)[0])\n",
        "                else:\n",
        "                    res.append(self.stoi.get(self._specials)[0])\n",
        "            return res\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Word {word} is not a string or a list of strings.\"\n",
        "                )\n",
        "\n",
        "\n",
        "    def get_freq(self, word: Union[str, List]):\n",
        "        if isinstance(word, str):\n",
        "            if word in self.stoi:\n",
        "                return self.stoi.get(word)[1]\n",
        "            else:\n",
        "                return self.stoi.get(self._specials)[1]\n",
        "        elif isinstance(word, list):\n",
        "            res = []\n",
        "            for w in word:\n",
        "                if w in self.stoi:\n",
        "                    res.append(self.stoi.get(w)[1])\n",
        "                else:\n",
        "                    res.append(self.stoi.get(self._specials)[1])\n",
        "            return res\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Word {word} is not a string or a list of strings.\"\n",
        "                )\n",
        "\n",
        "\n",
        "    def lookup_token(self, token: Union[int, List]):\n",
        "        if isinstance(token, (int, np.int64)):\n",
        "            if token in self.itos:\n",
        "                return self.itos.get(token)[0]\n",
        "            else:\n",
        "                raise ValueError(f\"Token {token} not in vocabulary\")\n",
        "        elif isinstance(token, list):\n",
        "            res = []\n",
        "            for t in token:\n",
        "                if t in self.itos:\n",
        "                    res.append(self.itos.get(token)[0])\n",
        "                else:\n",
        "                    raise ValueError(f\"Token {t} is not a valid index.\")\n",
        "            return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "id": "iJp-0yudpFfR"
      },
      "outputs": [],
      "source": [
        "def yield_tokens(iterator, tokenizer):\n",
        "    r = re.compile('[a-z1-9]')\n",
        "    for text in iterator:\n",
        "        res = tokenizer(text)\n",
        "        res = list(filter(r.match, res))\n",
        "        yield res\n",
        "\n",
        "def vocab(ordered_dict: Dict, min_freq: int = 1, specials: str = '<unk>'):\n",
        "    tokens = []\n",
        "    # Save room for special tokens\n",
        "    for token, freq in ordered_dict.items():\n",
        "        if freq >= min_freq:\n",
        "            tokens.append((token, freq))\n",
        "\n",
        "    specials = (specials, np.nan)\n",
        "    tokens[0] = specials\n",
        "\n",
        "    return Vocab(tokens, specials)\n",
        "\n",
        "def pipeline(word, vocab, tokenizer):\n",
        "    return vocab(tokenizer(word))\n",
        "\n",
        "def build_vocab(\n",
        "        iterator,\n",
        "        tokenizer,\n",
        "        params: Word2VecParams,\n",
        "        max_tokens: Optional[int] = None,\n",
        "    ):\n",
        "    counter = Counter()\n",
        "    for tokens in yield_tokens(iterator, tokenizer):\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # First sort by descending frequency, then lexicographically\n",
        "    sorted_by_freq_tuples = sorted(\n",
        "        counter.items(), key=lambda x: (-x[1], x[0])\n",
        "        )\n",
        "\n",
        "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "\n",
        "    # ordered_dict = OrderedDict(counter.items())\n",
        "\n",
        "    word_vocab = vocab(\n",
        "        ordered_dict, min_freq=params.MIN_FREQ, specials=params.SPECIALS\n",
        "        )\n",
        "    \n",
        "    return word_vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "V-5vtReKtlg0"
      },
      "outputs": [],
      "source": [
        "class SkipGrams:\n",
        "    def __init__(self, vocab: Vocab, vocab_context: Vocab, params: Word2VecParams, tokenizer):\n",
        "        self.vocab = vocab\n",
        "        self.vocab_context = vocab_context\n",
        "        self.params = params\n",
        "        self.t = self._t()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.discard_probs = self._create_discard_dict()\n",
        "\n",
        "    def _t(self):\n",
        "        freq_list = []\n",
        "        for _, (_, freq) in list(self.vocab.stoi.items())[1:]:\n",
        "            freq_list.append(freq/self.vocab.total_tokens)\n",
        "        return np.percentile(freq_list, self.params.T)\n",
        "\n",
        "\n",
        "    def _create_discard_dict(self):\n",
        "        discard_dict = {}\n",
        "        for _, (word, freq) in self.vocab.stoi.items():\n",
        "            dicard_prob = 1-np.sqrt(\n",
        "                self.t / (freq/self.vocab.total_tokens + self.t))\n",
        "            discard_dict[word] = dicard_prob\n",
        "        return discard_dict\n",
        "\n",
        "\n",
        "    def collate_skipgram(self, batch):\n",
        "        batch_input, batch_output  = [], []\n",
        "        for text in batch:\n",
        "            # Chestia asta e o lista cu index-ul fiecarui cuvant(al catelea cel mai frecvent e)\n",
        "            text_tokens = self.vocab.get_index(self.tokenizer(text))\n",
        "            text_tokens_context = []\n",
        "            for text in text_tokens:\n",
        "                context_word = self.vocab_context.lookup_token(text)\n",
        "                text_tokens_context.append(self.vocab_context.get_index(context_word))\n",
        "    \n",
        "            \n",
        "\n",
        "            if len(text_tokens_context) < self.params.SKIPGRAM_N_WORDS * 2 + 1:\n",
        "                continue\n",
        "\n",
        "            for idx in range(len(text_tokens_context) - self.params.SKIPGRAM_N_WORDS*2\n",
        "                ):\n",
        "                token_id_sequence = text_tokens[\n",
        "                    idx : (idx + self.params.SKIPGRAM_N_WORDS * 2 + 1)\n",
        "                    ]\n",
        "                \n",
        "                # Aici e scos cuvantul central, dar eu l-as pastra(pentru a il adauga pe cel din romana la context) si ca sa nu strict codul ii dau append dupa\n",
        "                input_ = token_id_sequence.pop(self.params.SKIPGRAM_N_WORDS)\n",
        "                outputs = token_id_sequence\n",
        "\n",
        "                # L-am adaugat inapoi aici\n",
        "                outputs.append(input_)\n",
        "\n",
        "\n",
        "                prb = random.random()\n",
        "                del_pair = self.discard_probs.get(input_)\n",
        "                if input_==0 or del_pair >= prb:\n",
        "                    continue\n",
        "                else:\n",
        "                    for output in outputs:\n",
        "                        prb = random.random()\n",
        "                        del_pair = self.discard_probs.get(output)\n",
        "                        if output==0 or del_pair >= prb:\n",
        "                            continue\n",
        "                        else:\n",
        "                            batch_input.append(input_)\n",
        "                            batch_output.append(output)\n",
        "\n",
        "        batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "        batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "\n",
        "        return batch_input, batch_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "0mTWsnXTxTXz"
      },
      "outputs": [],
      "source": [
        "class NegativeSampler:\n",
        "    def __init__(self, vocab: Vocab, ns_exponent: float, ns_array_len: int):\n",
        "        self.vocab = vocab\n",
        "        self.ns_exponent = ns_exponent\n",
        "        self.ns_array_len = ns_array_len\n",
        "        self.ns_array = self._create_negative_sampling()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ns_array)\n",
        "\n",
        "    def _create_negative_sampling(self):\n",
        "\n",
        "        frequency_dict = {word:freq**(self.ns_exponent) \\\n",
        "                          for _,(word, freq) in\n",
        "                          list(self.vocab.stoi.items())[1:]}\n",
        "        frequency_dict_scaled = {\n",
        "            word:\n",
        "            max(1,int((freq/self.vocab.total_tokens)*self.ns_array_len))\n",
        "            for word, freq in frequency_dict.items()\n",
        "            }\n",
        "        ns_array = []\n",
        "        for word, freq in tqdm(frequency_dict_scaled.items()):\n",
        "            ns_array = ns_array + [word]*freq\n",
        "        return ns_array\n",
        "\n",
        "    def sample(self,n_batches: int=1, n_samples: int=1):\n",
        "        samples = []\n",
        "        for _ in range(n_batches):\n",
        "            samples.append(random.sample(self.ns_array, n_samples))\n",
        "        samples = torch.as_tensor(np.array(samples))\n",
        "        return samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "id": "a-Xjg40wvtag"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab: Vocab, vocab_context:Vocab, params: Word2VecParams):\n",
        "        super().__init__()\n",
        "        self.vocab = vocab\n",
        "        self.vocab_context = vocab_context\n",
        "        self.t_embeddings = nn.Embedding(\n",
        "            self.vocab.__len__()+1,\n",
        "            params.EMBED_DIM,\n",
        "            max_norm=params.EMBED_MAX_NORM\n",
        "            )\n",
        "        self.c_embeddings = nn.Embedding(\n",
        "            self.vocab_context.__len__()+1,\n",
        "            params.EMBED_DIM,\n",
        "            max_norm=params.EMBED_MAX_NORM\n",
        "            )\n",
        "\n",
        "    def forward(self, inputs, context):\n",
        "        # getting embeddings for target & reshaping\n",
        "        target_embeddings = self.t_embeddings(inputs)\n",
        "        n_examples = target_embeddings.shape[0]\n",
        "        n_dimensions = target_embeddings.shape[1]\n",
        "        target_embeddings = target_embeddings.view(n_examples, 1, n_dimensions)\n",
        "\n",
        "        # get embeddings for context labels & reshaping\n",
        "        # Allows us to do a bunch of matrix multiplications\n",
        "        context_embeddings = self.c_embeddings(context)\n",
        "        # * This transposes each batch\n",
        "        context_embeddings = context_embeddings.permute(0,2,1)\n",
        "\n",
        "        # * custom linear layer\n",
        "        dots = target_embeddings.bmm(context_embeddings)\n",
        "        dots = dots.view(dots.shape[0], dots.shape[2])\n",
        "        return dots\n",
        "\n",
        "    def normalize_embeddings(self):\n",
        "        embeddings = list(self.t_embeddings.parameters())[0]\n",
        "        embeddings = embeddings.cpu().detach().numpy()\n",
        "        norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
        "        norms = norms.reshape(norms.shape[0], 1)\n",
        "        return embeddings / norms\n",
        "\n",
        "    def get_similar_words(self, word, n):\n",
        "        word_id = self.vocab.get_index(word)\n",
        "        if word_id == 0:\n",
        "            print(\"Out of vocabulary word\")\n",
        "            return\n",
        "\n",
        "        embedding_norms = self.normalize_embeddings()\n",
        "        word_vec = embedding_norms[word_id]\n",
        "        word_vec = np.reshape(word_vec, (word_vec.shape[0], 1))\n",
        "        dists = np.matmul(embedding_norms, word_vec).flatten()\n",
        "        topN_ids = np.argsort(-dists)[1 : n + 1]\n",
        "\n",
        "        topN_dict = {}\n",
        "        for sim_word_id in topN_ids:\n",
        "            sim_word = self.vocab_context.lookup_token(sim_word_id)\n",
        "            topN_dict[sim_word] = dists[sim_word_id]\n",
        "        return topN_dict\n",
        "\n",
        "    def get_similarity(self, word1, word2):\n",
        "        idx1 = self.vocab.get_index(word1)\n",
        "        idx2 = self.vocab_context.get_index(word2)\n",
        "        if idx1 == 0 or idx2 == 0:\n",
        "            print(\"One or both words are out of vocabulary\")\n",
        "            return\n",
        "\n",
        "        embedding_norms = self.normalize_embeddings()\n",
        "        word1_vec, word2_vec = embedding_norms[idx1], embedding_norms[idx2]\n",
        "\n",
        "        return cosine(word1_vec, word2_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "id": "mMx67oLYwS9x"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model: Model, params: Word2VecParams, optimizer,\n",
        "                vocab: Vocab, train_iter, valid_iter, skipgrams: SkipGrams):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.vocab = vocab\n",
        "        self.train_iter = train_iter\n",
        "        self.valid_iter = valid_iter\n",
        "        self.skipgrams = skipgrams\n",
        "        self.params = params\n",
        "\n",
        "        self.epoch_train_mins = {}\n",
        "        self.loss = {\"train\": [], \"valid\": []}\n",
        "\n",
        "        # sending all to device\n",
        "        self.model.to(self.params.DEVICE)\n",
        "        self.params.CRITERION.to(self.params.DEVICE)\n",
        "\n",
        "        self.negative_sampler = NegativeSampler(\n",
        "            vocab=self.vocab, ns_exponent=.75,\n",
        "            ns_array_len=self.params.NS_ARRAY_LEN\n",
        "            )\n",
        "        self.testwords = ['pe']\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        self.test_testwords()\n",
        "        for epoch in range(self.params.N_EPOCHS):\n",
        "            # Generate Dataloaders\n",
        "            self.train_dataloader = DataLoader(\n",
        "                self.train_iter,\n",
        "                batch_size=self.params.BATCH_SIZE,\n",
        "                shuffle=False,\n",
        "                collate_fn=self.skipgrams.collate_skipgram\n",
        "            )\n",
        "            self.valid_dataloader = DataLoader(\n",
        "                self.valid_iter,\n",
        "                batch_size=self.params.BATCH_SIZE,\n",
        "                shuffle=False,\n",
        "                collate_fn=self.skipgrams.collate_skipgram\n",
        "            )\n",
        "            # training the model\n",
        "            st_time = monotonic()\n",
        "            self._train_epoch()\n",
        "            self.epoch_train_mins[epoch] = round((monotonic()-st_time)/60, 1)\n",
        "\n",
        "            # validating the model\n",
        "            self._validate_epoch()\n",
        "            print(f\"\"\"Epoch: {epoch+1}/{self.params.N_EPOCHS}\\n\"\"\",\n",
        "            f\"\"\"    Train Loss: {self.loss['train'][-1]:.2}\\n\"\"\",\n",
        "            f\"\"\"    Valid Loss: {self.loss['valid'][-1]:.2}\\n\"\"\",\n",
        "            f\"\"\"    Training Time (mins): {self.epoch_train_mins.get(epoch)}\"\"\"\n",
        "            \"\"\"\\n\"\"\"\n",
        "            )\n",
        "            self.test_testwords()\n",
        "\n",
        "\n",
        "    def _train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = []\n",
        "\n",
        "        for i, batch_data in enumerate(self.train_dataloader, 1):\n",
        "            if len(batch_data[0]) == 0:\n",
        "                continue\n",
        "            inputs = batch_data[0].to(self.params.DEVICE)\n",
        "            pos_labels = batch_data[1].to(self.params.DEVICE)\n",
        "            neg_labels = self.negative_sampler.sample(\n",
        "                pos_labels.shape[0], self.params.NEG_SAMPLES\n",
        "                )\n",
        "            neg_labels = neg_labels.to(self.params.DEVICE)\n",
        "            context = torch.cat(\n",
        "                [pos_labels.view(pos_labels.shape[0], 1),\n",
        "                neg_labels], dim=1\n",
        "              )\n",
        "\n",
        "            # building the targets tensor\n",
        "            y_pos = torch.ones((pos_labels.shape[0], 1))\n",
        "            y_neg = torch.zeros((neg_labels.shape[0], neg_labels.shape[1]))\n",
        "            y = torch.cat([y_pos, y_neg], dim=1).to(self.params.DEVICE)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs, context)\n",
        "            loss = self.params.CRITERION(outputs, y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "        epoch_loss = np.mean(running_loss)\n",
        "\n",
        "        self.loss['train'].append(epoch_loss)\n",
        "\n",
        "    def _validate_epoch(self):\n",
        "        self.model.eval()\n",
        "        running_loss = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch_data in enumerate(self.valid_dataloader, 1):\n",
        "                if len(batch_data[0]) == 0:\n",
        "                    continue\n",
        "                inputs = batch_data[0].to(self.params.DEVICE)\n",
        "                pos_labels = batch_data[1].to(self.params.DEVICE)\n",
        "                neg_labels = self.negative_sampler.sample(\n",
        "                    pos_labels.shape[0], self.params.NEG_SAMPLES\n",
        "                    ).to(self.params.DEVICE)\n",
        "                context = torch.cat(\n",
        "                    [pos_labels.view(pos_labels.shape[0], 1),\n",
        "                    neg_labels], dim=1\n",
        "                  )\n",
        "\n",
        "\n",
        "                # building the targets tensor\n",
        "                y_pos = torch.ones((pos_labels.shape[0], 1))\n",
        "                y_neg = torch.zeros((neg_labels.shape[0], neg_labels.shape[1]))\n",
        "                y = torch.cat([y_pos, y_neg], dim=1).to(self.params.DEVICE)\n",
        "\n",
        "                preds = self.model(inputs, context).to(self.params.DEVICE)\n",
        "                loss = self.params.CRITERION(preds, y)\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "            epoch_loss = np.mean(running_loss)\n",
        "            self.loss['valid'].append(epoch_loss)\n",
        "\n",
        "    def test_testwords(self, n: int = 5):\n",
        "        for word in self.testwords:\n",
        "            print(word)\n",
        "            nn_words = self.model.get_similar_words(word, n)\n",
        "            for w, sim in nn_words.items():\n",
        "                print(f\"{w} ({sim:.3})\", end=' ')\n",
        "            print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "id": "q6DtwPHfqRVV"
      },
      "outputs": [],
      "source": [
        "params = Word2VecParams()\n",
        "train_iter, valid_iter = get_data()\n",
        "train_iter_context, valid_iter_context = get_data(train_dir='C:/Users/gheto/Desktop/PoS/AromanianPoS/dataset/Tales.train.rup', valid_dir='C:/Users/gheto/Desktop/PoS/AromanianPoS/dataset/Tales.test.rup')\n",
        "tokenizer = get_tokenizer(params.TOKENIZER)\n",
        "vocab_center = build_vocab(train_iter, tokenizer, params)\n",
        "vocab_context = build_vocab(train_iter_context, tokenizer, params)\n",
        "skip_gram = SkipGrams(vocab=vocab_center, vocab_context=vocab_context, params=params, tokenizer=tokenizer)\n",
        "model = Model(vocab=vocab_center, vocab_context=vocab_context, params=params).to(params.DEVICE)\n",
        "optimizer = torch.optim.Adam(params = model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63ZAFXnOxi32",
        "outputId": "c4d18e9a-2e63-4ea4-a690-fc236ce92485"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        params=params,\n",
        "        optimizer=optimizer,\n",
        "        train_iter=train_iter,\n",
        "        valid_iter=valid_iter,\n",
        "        vocab=vocab_context, # vocabularul de aici e folosit pentru negative sampling, pe care il facem din context\n",
        "        skipgrams=skip_gram\n",
        "    )\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pe\n",
            "păltări (0.207) calea-calea (0.199) padea (0.194) cu-alantu (0.186) pre-anarga (0.179) \n",
            "\n",
            "douăzeci\n",
            "nicaț (0.226) adunai (0.199) 1906 (0.194) cuibul (0.18) asvindzeam (0.18) \n",
            "\n",
            "mănâncă\n",
            "fapțîl’i (0.213) chetrile (0.198) tatălui (0.198) aistă (0.197) aușatic (0.197) \n",
            "\n",
            "penele\n",
            "armân (0.209) topcă (0.198) arșițile (0.192) pitrumsiră (0.191) aprindeț (0.188) \n",
            "\n",
            "băiete\n",
            "daț-le (0.208) tu-apirită (0.203) fumăria (0.192) asculți (0.188) harfă (0.188) \n",
            "\n",
            "furtuna\n",
            "se-andrupă (0.202) suflite (0.188) s-filipsească (0.187) singură (0.182) ficior-fic (0.176) \n",
            "\n",
            "zbura\n",
            "bag-u (0.195) di-aclo (0.189) armănea (0.188) arse (0.182) dorină (0.177) \n",
            "\n",
            "citind\n",
            "nîs“ (0.206) piricl’iu (0.204) dusiră (0.184) graiurî (0.18) juneaște (0.179) \n",
            "\n",
            "armânii\n",
            "bîna (0.197) nîs (0.195) acățață (0.186) dări (0.184) hapse (0.179) \n",
            "\n",
            "oile\n",
            "ancl’igat (0.211) ponda (0.202) pri-aclo (0.187) altă-oară (0.184) mul’erle (0.176) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_testwords(self, n: int = 5):\n",
        "    for word in [\"pe\", \"douăzeci\", \"mănâncă\", \"penele\", \"băiete\", \"furtuna\", \"zbura\", \"citind\", \"armânii\", \"oile\"]:\n",
        "        print(word)\n",
        "        nn_words = self.model.get_similar_words(word, n)\n",
        "        for w, sim in nn_words.items():\n",
        "            print(f\"{w} ({sim:.3})\", end=' ')\n",
        "        print('\\n') \n",
        "\n",
        "test_testwords(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pe\n",
            "chitroasă (0.213) vrură (0.201) lire (0.2) agioclu (0.196) hearele (0.194) \n",
            "\n",
            "douăzeci\n",
            "turma (0.224) stole (0.199) guli (0.196) chiñi (0.191) s-le-aibă (0.187) \n",
            "\n",
            "mănâncă\n",
            "mire (0.211) avhil’eate (0.207) s-minduia (0.19) l’epur (0.188) de-asime (0.187) \n",
            "\n",
            "penele\n",
            "ciuciteaște (0.218) lu-agiungu (0.201) ambar (0.198) alăgară (0.197) stres-stres (0.187) \n",
            "\n",
            "băiete\n",
            "zulăchilor (0.232) vilendză (0.23) mîcate (0.224) iu-ț (0.203) plăteaște (0.197) \n",
            "\n",
            "furtuna\n",
            "cutie (0.249) bunu (0.202) mîcară (0.194) n-afla (0.191) tăl’eat (0.183) \n",
            "\n",
            "zbura\n",
            "s-cutrimburară (0.225) s-γină“ (0.209) s-aungă (0.209) salți (0.199) angreacă (0.197) \n",
            "\n",
            "citind\n",
            "bărbărută (0.232) alîndurle (0.216) cot (0.211) tălăgane (0.209) cuțute (0.208) \n",
            "\n",
            "armânii\n",
            "a-nveastil’ei (0.191) căftară (0.187) avea-ntunicată (0.185) cărave (0.183) stihio (0.172) \n",
            "\n",
            "oile\n",
            "fufulii (0.248) paradhis (0.198) mîrînγipsite (0.188) bucură-te (0.182) dizlichi (0.181) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_testwords(self, n: int = 5):\n",
        "    for word in [\"pe\", \"douăzeci\", \"mănâncă\", \"penele\", \"băiete\", \"furtuna\", \"zbura\", \"citind\", \"armânii\", \"oile\"]:\n",
        "        print(word)\n",
        "        nn_words = self.model.get_similar_words(word, n)\n",
        "        for w, sim in nn_words.items():\n",
        "            print(f\"{w} ({sim:.3})\", end=' ')\n",
        "        print('\\n') \n",
        "\n",
        "test_testwords(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Doar niste experimente\n",
        "text_tokens = vocab_center.get_index(tokenizer(\"Sunt unele deprinderi pe care le câștigi doar citind, ori vorbindu-ți-se despre ele. Dar mai sunt și din cele ce nu ne intră în cap, până nu le vedem cu ochii. Cât trăim - multe auzim și multe vedem, dar cu toate astea - nimic nu vom ști ca lumea până nu punem mâna să și facem ceea ce ne pare că știm. Cu alte cuvinte, vreau să spun eu, teoria e bună ea, deseori, dar câte o dată rămâne de căruță față de practică. Așa grăia într-un rând un bătrân înțelept - către niște tineri ce încă nu ieșiseră din școală, și care își făceau ideea cum că... nimeni nu-i mai învățat ca ei, și că... tot ce zboară, se mănâncă! Ca să pricepeți ce vă spusei până aici, luați\"))\n",
        "print(text_tokens)\n",
        "context_words = []\n",
        "context_tokens = []\n",
        "for text in text_tokens:\n",
        "    context_word = vocab_context.lookup_token(text)\n",
        "    context_words.append(context_word)\n",
        "    context_tokens.append(vocab_context.get_index(context_word))\n",
        "print(context_tokens)\n",
        "print(context_words)\n",
        "\n",
        "print(tokenizer(\"TEORIA SI PRACTICA \\nSunt unele deprinderi pe care le câștigi doar citind, ori vorbindu-ți-se despre ele.\"))\n",
        "print(tokenizer(\"TEORIA SI PRACTICA \\nSănt îndoauă învețuri, cari si amintă mași cu ghivăsirea, i cu avdzărea.\"))\n",
        "\n",
        "print(vocab_center.get_index(\"sunt\"))\n",
        "print(vocab_context.get_index(\"sănt\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
